# -*- coding: utf-8 -*-
"""Laboratorio [OpenAI con LangChain].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12l9iZ_lwt0if4C7YkoYvUeWNl97VVlNG
"""

## #######################################################################################################
##
## @copyright Big Data Academy [info@bigdataacademy.org]
## @professor Alonso Melgarejo [alonsoraulmgs@gmail.com]
##
## #######################################################################################################

"""# 1. Instalación de Librerías"""

#Librería de OpenAI

"""!pip install openai"""

#Librería para implementar la arquitectura RAG

"""!pip install langchain"""

#Librería plugin para que Langchain pueda usar OpenAI

"""!pip install langchain-openai"""

"""# 2. Conexión al modelo"""

#Librería pip install python-dotenv  para que no se vea la API

from dotenv import load_dotenv
import os

load_dotenv()  # carga las variables del archivo .env
api_key = os.getenv("OPENAI_API_KEY")

#Utilitario de conexión OpenAI
from langchain_openai import ChatOpenAI

#Conexión a un modelo
llm = ChatOpenAI(
  model = "gpt-4.1",
  openai_api_key=api_key,
  temperature = 0.2,
)

"""# 4. Configuración del modelo"""

#"Personalidad" (contexto general) del modelo
contextoGeneral = """
  Eres un asistente llamado "BDA" que atenderá consultas de alumnos en una academia llamada "Big Data Academy", de
  formación de cursos de Big Data, Cloud Computing e Inteligencia Artificial. Al contestar debes
  seguir las siguientes reglas:

  1. Debes contestar en un lenguaje formal pero amigable
  2. Debes de usar emojis al responder
  3. No debes responder cosas no relacionadas con Big Data, Cloud Computing o Inteligencia Artificial
"""

#Utilitario para crear la memoria a corto plazo del modelo
from langchain.memory import ConversationBufferMemory

#Creamos la memoria a corto plazo
memoria = ConversationBufferMemory()

#Agregamos la "personalidad" a nuestra IA
memoria.chat_memory.add_ai_message(contextoGeneral)

"""# 5. Configuración del chat con el modelo"""

#Utilitario para crear una conversación de chat con el modelo
from langchain.chains import ConversationChain

#Creamos la conversación de chat
chat = ConversationChain(
    llm = llm,
    memory = memoria,
    verbose = False #Desactivamos el log para ver sólo las respuestas del modelo
)

"""# 6. Uso del modelo"""

#Envíamos un mensaje
respuesta = chat.predict(input = """
Muy buenos días, estoy interesado en llevar cursos con ustedes.
¿Qué cursos tienen?, qué tipo de cursos tienen???
""")

#Vemos la respuesta
print(respuesta)

#Envíamos un mensaje
respuesta = chat.predict(input = """
Dame un dato interesante del fútbol
""")

#Vemos la respuesta
print(respuesta)

#Envíamos un mensaje
respuesta = chat.predict(input = """
Dame un resumen de la conversación
""")

#Vemos la respuesta
print(respuesta)